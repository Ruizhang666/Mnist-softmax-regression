{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train = True):\n",
    "    \"\"\"\n",
    "    Load the data from disk\n",
    "    Parameters\n",
    "    ----------\n",
    "    train : bool\n",
    "        Load training data if true, else load test data\n",
    "    Returns\n",
    "    -------\n",
    "        Tuple:\n",
    "            Images\n",
    "            Labels\n",
    "    \"\"\"\n",
    "    directory = 'train' if train else 'test'\n",
    "    patterns = np.load(os.path.join('./data/', directory, 'images.npz'))['arr_0']\n",
    "    labels = np.load(os.path.join('./data/', directory, 'labels.npz'))['arr_0']\n",
    "    return patterns.reshape(len(patterns), -1), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_minibatches(dataset, batch_size=64):\n",
    "    X, y = dataset\n",
    "    l_idx, r_idx = 0, batch_size\n",
    "    while r_idx < len(X):\n",
    "        yield X[l_idx:r_idx], y[l_idx:r_idx]\n",
    "        l_idx, r_idx = r_idx, r_idx + batch_size\n",
    "\n",
    "    yield X[l_idx:], y[l_idx:]\n",
    "\n",
    "def generate_k_fold_set(dataset, k = 10): \n",
    "    X, y = dataset\n",
    "\n",
    "    order = np.random.permutation(len(X))\n",
    "    \n",
    "    fold_width = len(X) // k\n",
    "\n",
    "    l_idx, r_idx = 0, fold_width\n",
    "\n",
    "    for i in range(k):\n",
    "        train = np.concatenate([X[order[:l_idx]], X[order[r_idx:]]]), np.concatenate([y[order[:l_idx]], y[order[r_idx:]]])\n",
    "        validation = X[order[l_idx:r_idx]], y[order[l_idx:r_idx]]\n",
    "        yield train, validation\n",
    "        l_idx, r_idx = r_idx, r_idx + fold_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=load_data(train = True)\n",
    "test=load_data(train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_normalize(X, u = None, xd = None):\n",
    "    \"\"\"\n",
    "    Performs z-score normalization on X. \n",
    "    f(x) = (x - μ) / σ\n",
    "        where \n",
    "            μ = mean of x\n",
    "            σ = standard deviation of x\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        The data to z-score normalize\n",
    "    u (optional) : np.array\n",
    "        The mean to use when normalizing\n",
    "    sd (optional) : np.array\n",
    "        The standard deviation to use when normalizing\n",
    "    Returns\n",
    "    -------\n",
    "        Tuple:\n",
    "            Transformed dataset with mean 0 and stdev 1\n",
    "            Computed statistics (mean and stdev) for the dataset to undo z-scoring.\n",
    "    \"\"\"\n",
    "    if u == None:\n",
    "        mean=np.mean(X, axis=0)\n",
    "    else:\n",
    "        mean=u\n",
    "    if xd==None:\n",
    "        std=np.std(X, axis=0)\n",
    "    else:\n",
    "        std=xd\n",
    "        \n",
    "    X = (X - mean) / std\n",
    "    \n",
    "    return (X,mean,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(X, _min = None, _max = None):\n",
    "    \"\"\"\n",
    "    Performs min-max normalization on X. \n",
    "    f(x) = (x - min(x)) / (max(x) - min(x))\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "        The data to min-max normalize\n",
    "    _min (optional) : np.array\n",
    "        The min to use when normalizing\n",
    "    _max (optional) : np.array\n",
    "        The max to use when normalizing\n",
    "    Returns\n",
    "    -------\n",
    "        Tuple:\n",
    "            Transformed dataset with all values in [0,1]\n",
    "            Computed statistics (min and max) for the dataset to undo min-max normalization.\n",
    "    \"\"\"\n",
    "    if _min == None:\n",
    "        _min=np.min(X,axis=0)\n",
    "    if _max == None:\n",
    "        _max=np.max(X,axis=0)\n",
    "\n",
    "        \n",
    "    X= (X-_min)/(_max - _min)\n",
    "    \n",
    "    return (X,_min,_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(y):\n",
    "    \"\"\"\n",
    "    Performs one-hot encoding on y.\n",
    "    Ideas:\n",
    "        NumPy's `eye` function\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        1d array (length n) of targets (k)\n",
    "    Returns\n",
    "    -------\n",
    "        2d array (shape n*k) with each row corresponding to a one-hot encoded version of the original value.\n",
    "    \"\"\"\n",
    "    if len(np.unique(y))>2:\n",
    "        values = y\n",
    "        n_values = np.max(values) + 1\n",
    "        return np.eye(n_values)[values] \n",
    "    else:\n",
    "        value = np.max(y)\n",
    "        return (y == value).astype(int).reshape(len(y),1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(dataset):\n",
    "    \"\"\"\n",
    "    Shuffle dataset.\n",
    "    Make sure that corresponding images and labels are kept together. \n",
    "    Ideas: \n",
    "        NumPy array indexing \n",
    "            https://numpy.org/doc/stable/user/basics.indexing.html#advanced-indexing\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset\n",
    "        Tuple containing\n",
    "            Images (X)\n",
    "            Labels (y)\n",
    "    Returns\n",
    "    -------\n",
    "        Tuple containing\n",
    "            Images (X)\n",
    "            Labels (y)\n",
    "    \"\"\"\n",
    "    dim=dataset[0].shape[0]\n",
    "    index1=np.arange(dim)\n",
    "    np.random.shuffle(index1)\n",
    "    shuffled_data=dataset[0][index1]\n",
    "    shuffled_label=dataset[1][index1]\n",
    "    return (shuffled_data,shuffled_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_bias(X):\n",
    "    \"\"\"\n",
    "    Append bias term for dataset.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X\n",
    "        2d numpy array with shape (N,d)\n",
    "    Returns\n",
    "    -------\n",
    "        2d numpy array with shape ((N+1),d)\n",
    "    \"\"\"\n",
    "    new=np.ones((X.shape[0],X.shape[1]+1))\n",
    "    new[:,:-1] = X\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function.\n",
    "\n",
    "    f(x) = 1 / (1 + e ^ (-x))\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a\n",
    "        The internal value while a pattern goes through the network\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "       Value after applying sigmoid (z from the slides).\n",
    "    \"\"\"\n",
    "    clipped=[]\n",
    "    a=a.flatten()\n",
    "    for i in range(len(a)):\n",
    "        if a[i]>20:\n",
    "            clipped.append(20)\n",
    "        elif a[i]<-20:\n",
    "            clipped.append(-20)\n",
    "        else:\n",
    "            clipped.append(a[i])\n",
    "    clipped=np.array(clipped)\n",
    "    return 1/(1+np.exp(-clipped))\n",
    "\n",
    "def softmax(a):\n",
    "    \"\"\"\n",
    "    Compute the softmax function.\n",
    "\n",
    "    f(x) = (e^x) / Σ (e^x)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a\n",
    "        The internal value while a pattern goes through the network\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "       Value after applying softmax (z from the slides).\n",
    "    \"\"\"\n",
    "#     a=np.clip(a,-20,20)\n",
    "#     denominator=0\n",
    "#     for i in range(a.shape[1]):\n",
    "#         denominator+=np.exp(a[:,i])\n",
    "#     result=[]\n",
    "#     for i in range(a.shape[1]):\n",
    "#         result.append(np.exp(a[:,i])/denominator)\n",
    "        \n",
    "#     return np.array(result).T\n",
    "    a=np.clip(a,-20,20)\n",
    "    a_exp = np.exp(a)\n",
    "    partition = np.sum(a_exp, axis=1).reshape(-1,1)\n",
    "    return a_exp / partition\n",
    "\n",
    "def binary_cross_entropy(y, t):\n",
    "    \"\"\"\n",
    "    Compute binary cross entropy.\n",
    "\n",
    "    L(x) = t*ln(y) + (1-t)*ln(1-y)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y\n",
    "        The network's predictions\n",
    "    t\n",
    "        The corresponding targets\n",
    "    Returns\n",
    "    -------\n",
    "    float \n",
    "        binary cross entropy loss value according to above definition\n",
    "    \"\"\"\n",
    "    vectorized=t*np.log(y)+(1-t)*np.log(1-y)\n",
    "    return -np.mean(vectorized)\n",
    "\n",
    "def multiclass_cross_entropy(y, t):\n",
    "    \"\"\"\n",
    "    Compute multiclass cross entropy.\n",
    "\n",
    "    L(x) = - Σ (t*ln(y))\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y\n",
    "        The network's predictions\n",
    "    t\n",
    "        The corresponding targets\n",
    "    Returns\n",
    "    -------\n",
    "    float \n",
    "        multiclass cross entropy loss value according to above definition\n",
    "    \"\"\"\n",
    "    entropy=np.zeros(t.shape[0])\n",
    "    for i in range(10):\n",
    "        target_column=t[:,i]\n",
    "        prediction_column=y[:,i]\n",
    "        entropy+=target_column*np.log(prediction_column)\n",
    "    return -np.mean(entropy/10)\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, hyperparameters, activation, loss, out_dim):\n",
    "        \"\"\"\n",
    "        Perform required setup for the network.\n",
    "\n",
    "        Initialize the weight matrix, set the activation function, save hyperparameters.\n",
    "\n",
    "        You may want to create arrays to save the loss values during training.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hyperparameters\n",
    "            A Namespace object from `argparse` containing the hyperparameters\n",
    "        activation\n",
    "            The non-linear activation function to use for the network\n",
    "        loss\n",
    "            The loss function to use while training and testing\n",
    "        \"\"\"\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.out_dim=out_dim\n",
    "\n",
    "        self.weights = np.zeros((28*28+1, out_dim))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Apply the model to the given patterns\n",
    "\n",
    "        Use `self.weights` and `self.activation` to compute the network's output\n",
    "\n",
    "        f(x) = σ(w*x)\n",
    "            where\n",
    "                σ = non-linear activation function\n",
    "                w = weight matrix\n",
    "\n",
    "        Make sure you are using matrix multiplication when you vectorize your code!\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X\n",
    "            Patterns to create outputs for\n",
    "            \n",
    "        \"\"\"\n",
    "        product=np.matmul(X,self.weights)\n",
    "        return self.activation(product)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "    def train(self, minibatch):\n",
    "        \"\"\"\n",
    "        Train the network on the given minibatch\n",
    "\n",
    "        Use `self.weights` and `self.activation` to compute the network's output\n",
    "        Use `self.loss` and the gradient defined in the slides to update the network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        minibatch\n",
    "            The minibatch to iterate over\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple containing:\n",
    "            average loss over minibatch\n",
    "            accuracy over minibatch\n",
    "        \"\"\"\n",
    "        X, y = minibatch\n",
    "        \n",
    "        # To train the model, we have several things to do:\n",
    "        # We will read of the learning rate, and bath size.\n",
    "        # Then, based on the specification of the user,normalize the data.\n",
    "        \n",
    "        learning_rate=self.hyperparameters[0]\n",
    "        batch_size_x=self.hyperparameters[1]\n",
    "        \n",
    "        X,mean_X,std_X=self.hyperparameters[2](X) # data Normalization and append bias \n",
    "        X=append_bias(X)\n",
    "        \n",
    "        \n",
    "        dataset=(X,y)\n",
    "        \n",
    "        # Keep track of our model performance over epoches \n",
    "        accuracies_10= []\n",
    "        training_losses_10 = []\n",
    "        validation_losses_10=[]\n",
    "        val_accuracies_10=[]\n",
    "        \n",
    "        for training, validation in generate_k_fold_set(dataset):\n",
    "            \n",
    "            self.weights = np.zeros((28*28+1, self.out_dim))\n",
    "            \n",
    "            #Keep track of the statistics for the single val process\n",
    "            accuracies = []\n",
    "            training_losses = []\n",
    "            validation_losses=[]\n",
    "            val_accuracies=[]\n",
    "            \n",
    "            # train our model with maximum 100 epoches allowed \n",
    "            for epoch in range(18):\n",
    "                training=shuffle(training)\n",
    "                # Generate mini_bathch with specified \n",
    "                for X_train,y_train in generate_minibatches(training,batch_size=batch_size_x):\n",
    "\n",
    "                    # Compute the output of our model with given weight\n",
    "                    # This should output a 10*data_size matrix, \n",
    "                    # where each column represents probability distribution\n",
    "\n",
    "                    predictions=self.forward(X_train)#.reshape((-1,self.out_dim))\n",
    "                    \n",
    "                    \n",
    "    \n",
    "                    # One-hot encode the labels for which we can compute the error rate \n",
    "                    # Notice that the dimension of the one-hot encoded target is datasize*10\n",
    "                    target=onehot_encode(y_train)#.reshape((-1,self.out_dim))\n",
    "                     \n",
    "      \n",
    "                    error_signal=(target-predictions)\n",
    "        \n",
    "            \n",
    "                    gradient=-1*(np.matmul(error_signal.T,X_train))\n",
    "                    #gradient=gradient.flatten().reshape((-1,self.out_dim))\n",
    "                    \n",
    "                \n",
    "                    # Upadte the weight matrix with gradient descent\n",
    "                    \n",
    "                    self.weights=self.weights-learning_rate*gradient.T\n",
    "                          \n",
    "\n",
    "                # Evaluate our model's performance at the end of each epoches\n",
    "                # Evaluate at the entire training dataset.\n",
    "                #print(training[0])\n",
    "                prediction_epoch=self.forward(training[0])\n",
    "                \n",
    "                if self.activation==softmax:\n",
    "                    encoded_target=onehot_encode(training[1])\n",
    "                    training_losses_n=multiclass_cross_entropy(prediction_epoch,encoded_target)\n",
    "                    training_losses.append(training_losses_n)\n",
    "                    # compute the accuracy\n",
    "                    predicted_labels=np.argmax(prediction_epoch,axis=1)\n",
    "                    labels=np.argmax(encoded_target,axis=1)\n",
    "                    single_accuracy=np.mean(predicted_labels==labels)\n",
    "                    print(single_accuracy)\n",
    "                    accuracies.append(single_accuracy)\n",
    "                else:\n",
    "                    encoded_target=onehot_encode(training[1]).flatten()\n",
    "                    training_losses_n=binary_cross_entropy(prediction_epoch,encoded_target)\n",
    "                    training_losses.append(training_losses_n)\n",
    "                    #print(training_losses_n)\n",
    "                    # compute the accuracy \n",
    "                    predicted_labels=1*(prediction_epoch>0.5)\n",
    "                    single_accuracy=np.mean(predicted_labels==encoded_target)\n",
    "                    accuracies.append(single_accuracy)\n",
    "                    \n",
    "                # Keep track of the validation losses \n",
    "                val_predict=self.forward(validation[0])\n",
    "                encoded_val_target=onehot_encode(validation[1])\n",
    "\n",
    "                if self.activation==softmax:\n",
    "                    val_losses=multiclass_cross_entropy(val_predict,encoded_val_target)\n",
    "                    validation_losses.append(val_losses)\n",
    "                    # compute the accuracy\n",
    "                    predicted_labels_val=np.argmax(val_predict,axis=1)\n",
    "                    labels_val=np.argmax(encoded_val_target,axis=1)\n",
    "                    single_accuracy_val=np.mean(predicted_labels_val==labels_val)\n",
    "                    val_accuracies.append(single_accuracy_val)\n",
    "                else:\n",
    "                    encoded_val_target=onehot_encode(validation[1]).flatten()\n",
    "                    val_losses=binary_cross_entropy(val_predict,encoded_val_target)\n",
    "                    validation_losses.append(val_losses) \n",
    "                    predicted_labels_val=1*(val_predict>0.5)\n",
    "                    single_accuracy_val=np.mean(predicted_labels_val==encoded_val_target)\n",
    "                    #print(single_accuracy_val)\n",
    "                    val_accuracies.append(single_accuracy_val)\n",
    "                    \n",
    "                \n",
    "            accuracies_10.append(accuracies)\n",
    "            training_losses_10.append(training_losses)\n",
    "            validation_losses_10.append(validation_losses)\n",
    "            val_accuracies_10.append(val_accuracies)    \n",
    "            \n",
    "        return accuracies_10,training_losses_10,validation_losses_10,val_accuracies_10\n",
    "                \n",
    "\n",
    "    def test(self, minibatch):\n",
    "        \"\"\"\n",
    "        Test the network on the given minibatch\n",
    "\n",
    "        Use `self.weights` and `self.activation` to compute the network's output\n",
    "        Use `self.loss` to compute the loss.\n",
    "        Do NOT update the weights in this method!\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        minibatch\n",
    "            The minibatch to iterate over\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            tuple containing:\n",
    "                average loss over minibatch\n",
    "                accuracy over minibatch\n",
    "        \"\"\"\n",
    "        X, y = minibatch\n",
    "        # Remember to normalize the data and append bias \n",
    "        X,mean_X,std_X=self.hyperparameters[2](X)\n",
    "        X=append_bias(X)\n",
    "        prediction_test=self.forward(X)\n",
    "        encoded_target=onehot_encode(y).flatten()\n",
    "        accuracy=None\n",
    "        losses=None\n",
    "        \n",
    "        if self.activation==softmax:\n",
    "            losses=multiclass_cross_entropy(prediction_test,encoded_target)\n",
    "            # compute the accuracy\n",
    "            predicted_labels_test=np.argmax(prediction_test,axis=1)\n",
    "            labels_test=np.argmax(encoded_target,axis=1)\n",
    "            accuracy=np.mean(predicted_labels_test==labels_test)\n",
    "        else:\n",
    "            losses=binary_cross_entropy(prediction_test,encoded_target)\n",
    "            # compute the accuracy \n",
    "            predicted_labels_test=1*(prediction_test>0.5)\n",
    "            accuracy=np.mean(predicted_labels_test==encoded_target)\n",
    "        return accuracy,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_1=[0.001,300,z_score_normalize]\n",
    "hyperparameters_2=[0.01,300,min_max_normalize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for logistic regression \n",
    "data_0=train[0][train[1]==2]\n",
    "labels_0=train[1][train[1]==2]\n",
    "data_6=train[0][train[1]==6]\n",
    "labels_6=train[1][train[1]==6]\n",
    "test_0=test[0][test[1]==2]\n",
    "test_labels_0=test[1][test[1]==2]\n",
    "test_6=test[0][test[1]==6]\n",
    "test_labels_6=test[1][test[1]==6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_logistic=np.array(list(data_0)+list(data_6))\n",
    "label_logistic=np.array(list(labels_0)+list(labels_6))\n",
    "logistic_dataset=(data_logistic,label_logistic)\n",
    "test_logistic=np.array(list(test_0)+list(test_6))\n",
    "test_label_logistic=np.array(list(test_labels_0)+list(test_labels_6))\n",
    "logistic_test=(test_logistic,test_label_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression=Network(hyperparameters_1,sigmoid, binary_cross_entropy,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (785,300) (785,90000) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-a5f05a884944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-99-9f54416706b6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, minibatch)\u001b[0m\n\u001b[1;32m    226\u001b[0m                     \u001b[0;31m# Upadte the weight matrix with gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (785,300) (785,90000) "
     ]
    }
   ],
   "source": [
    "logistic_regression.train(logistic_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic_regression.test(logistic_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hyperparameters_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fe4a9365cdb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msoftmax_regression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameters_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulticlass_cross_entropy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'hyperparameters_1' is not defined"
     ]
    }
   ],
   "source": [
    "softmax_regression=Network(hyperparameters_1,softmax, multiclass_cross_entropy,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8186666666666667\n",
      "0.8247037037037037\n",
      "0.8237037037037037\n",
      "0.8296666666666667\n",
      "0.8277592592592593\n",
      "0.8263148148148148\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-cdde2e3add52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msoftmax_regression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-102-9f54416706b6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, minibatch)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;31m# train our model with maximum 100 epoches allowed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m                 \u001b[0;31m# Generate mini_bathch with specified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-ac6780956a5e>\u001b[0m in \u001b[0;36mshuffle\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mindex1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mshuffled_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mshuffled_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mshuffled_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffled_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "softmax_regression.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
